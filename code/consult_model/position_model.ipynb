{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rb58853/ML-RSI-Images/blob/main/code/consult_model/position_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "Lm8_tPwj8Bbe"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Input, Dense, Reshape, Flatten,LSTM\n",
        "from keras.models import Model,Sequential\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MyModel():\n",
        "  def __init__(self, data_path, max_input_len = 50, max_output_len = 50):\n",
        "    self.max_input_len = max_input_len\n",
        "    self.max_output_len = max_output_len\n",
        "\n",
        "    self.data = self.get_data(data_path)\n",
        "    self.model = self.attention_model()\n",
        "\n",
        "  def get_data(self,data_path):\n",
        "    return {\n",
        "    'input':[\n",
        "        'a dog to the left of a cat',\n",
        "        'a cat to the left of a lion',\n",
        "        'a bed to the left of a table',\n",
        "        'a sit to the left of a mantis',\n",
        "        'a object to the left of another object',\n",
        "        'a rock to the left of the water',\n",
        "        'a sea to the left of a shark'\n",
        "        ],\n",
        "    'output':[\n",
        "        ['a cat', 'a dog', '','',''],\n",
        "        ['a lion', 'a cat', '','',''],\n",
        "        ['a table', 'a bed', '','',''],\n",
        "        ['a mantis', 'a sit', '','',''],\n",
        "        ['another object', 'a object', '','',''],\n",
        "        ['the water', 'a rock', '','',''],\n",
        "        ['a shark', 'a sea', '','',''],\n",
        "        ],\n",
        "    'types':\n",
        "     [\n",
        "        [1,5,1,1,10,1,1,5],\n",
        "        [1,5,1,1,10,1,1,5],\n",
        "        [1,5,1,1,10,1,1,5],\n",
        "        [1,5,1,1,10,1,1,5],\n",
        "        [1,5,1,1,10,1,5,5],\n",
        "        [1,5,1,1,10,1,1,5],\n",
        "        [1,5,1,1,10,1,1,5],\n",
        "        ]\n",
        "      }\n",
        "\n",
        "  def get_train(self):\n",
        "    # Crea un objeto Tokenizer\n",
        "    tokenizer = Tokenizer()\n",
        "    # Ajusta el tokenizer a tus datos\n",
        "    types =  self.data['types']\n",
        "    output = self.data ['output']\n",
        "    input =  self.data ['input']\n",
        "\n",
        "    tokenizer.fit_on_texts(input+[':'])\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    sequences_input = tokenizer.texts_to_sequences(input)\n",
        "\n",
        "    sequences_output = []\n",
        "    for item in output:\n",
        "      texts = [text for text in item]\n",
        "      sequences_output.append(tokenizer.texts_to_sequences(texts))\n",
        "\n",
        "    # train_x =  [[date_input, date_type] for date_input, date_type in zip(sequences_input, types)]\n",
        "    train_x =  {'values': types,\n",
        "                'querys': sequences_input}\n",
        "\n",
        "    train_y = [item for item in sequences_output]\n",
        "\n",
        "    for key in train_x:\n",
        "      for item in train_x[key]:\n",
        "        while len(item) < self.max_input_len:\n",
        "          item.append(0)\n",
        "\n",
        "    for case_ in train_y:\n",
        "      for item in case_:\n",
        "        while len(item) < self.max_output_len:\n",
        "          item.append(0)\n",
        "\n",
        "    values = train_x['values']\n",
        "    train_x['values']= np.array([np.array(value) for value in values])\n",
        "    querys = train_x['querys']\n",
        "    train_x['querys']= np.array([np.array(query) for query in querys])\n",
        "    train_y = np.array([np.array(y) for y in train_y])\n",
        "\n",
        "    return  {'train_x':train_x, 'train_y':train_y}\n",
        "\n",
        "  def attention_model(self):\n",
        "    # Variable-length int sequences.\n",
        "    query_input = tf.keras.Input(shape=(self.max_input_len,), dtype='int32')\n",
        "    value_input = tf.keras.Input(shape=(self.max_input_len,), dtype='int32')\n",
        "\n",
        "    # Embedding lookup.\n",
        "    token_embedding = tf.keras.layers.Embedding(input_dim=self.max_input_len, output_dim=64)\n",
        "    # Query embeddings of shape [batch_size, Tq, dimension].\n",
        "    query_embeddings = token_embedding(query_input)\n",
        "    # Value embeddings of shape [batch_size, Tv, dimension].\n",
        "    value_embeddings = token_embedding(value_input)\n",
        "\n",
        "    # CNN layer.\n",
        "    cnn_layer = tf.keras.layers.Conv1D(\n",
        "        filters=100,\n",
        "        kernel_size=4,\n",
        "        # Use 'same' padding so outputs have the same shape as inputs.\n",
        "        padding='same')\n",
        "    # Query encoding of shape [batch_size, Tq, filters].\n",
        "    query_seq_encoding = cnn_layer(query_embeddings)\n",
        "    # Value encoding of shape [batch_size, Tv, filters].\n",
        "    value_seq_encoding = cnn_layer(value_embeddings)\n",
        "\n",
        "    # Query-value attention of shape [batch_size, Tq, filters].\n",
        "    query_value_attention_seq = tf.keras.layers.Attention()(\n",
        "        [query_seq_encoding, value_seq_encoding])\n",
        "\n",
        "    # Reduce over the sequence axis to produce encodings of shape\n",
        "    # [batch_size, filters].\n",
        "    query_encoding = tf.keras.layers.GlobalAveragePooling1D()(\n",
        "        query_seq_encoding)\n",
        "    query_value_attention = tf.keras.layers.GlobalAveragePooling1D()(\n",
        "        query_value_attention_seq)\n",
        "\n",
        "    # Concatenate query and document encodings to produce a DNN input layer.\n",
        "    input_layer = tf.keras.layers.Concatenate()(\n",
        "        [query_encoding, query_value_attention])\n",
        "\n",
        "    # Add DNN layers, and create Model.\n",
        "    len_output = self.max_output_len * 5\n",
        "\n",
        "    layer = Dense(64, activation='relu')(input_layer)\n",
        "    layer = Dense(len_output, activation='relu')(layer)\n",
        "    output_layer = Reshape((5, self.max_output_len))(layer)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[query_input, value_input], outputs=output_layer)\n",
        "    return model\n",
        "\n",
        "  def rnn_model(self):\n",
        "    total_len_output = self.max_output_len * 5\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(2, self.max_input_len)))\n",
        "    model.add(LSTM(256, activation='relu', return_sequences=True, input_shape=(100, self.max_input_len)))\n",
        "    model.add(LSTM(256, activation='relu', return_sequences=True, input_shape=(100, self.max_input_len)))\n",
        "    model.add(LSTM(256, activation='relu', return_sequences=True, input_shape=(100, self.max_input_len)))\n",
        "    model.add(LSTM(256, activation='relu'))\n",
        "    # model.add(Dense(128, activation='relu'))\n",
        "    # model.add(Dense(64, activation='relu'))\n",
        "    model.add(Flatten(input_shape=(0,64)))\n",
        "    model.add(Dense(total_len_output, activation='relu'))\n",
        "    model.add(Reshape((5, self.max_output_len)))\n",
        "    return model\n",
        "\n",
        "  def fit(self,epochs, batch_size=32):\n",
        "    self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    train = self.get_train()\n",
        "    train_x = [train['train_x']['querys'],train['train_x']['values']]\n",
        "    train_y = train['train_y']\n",
        "\n",
        "    self.model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\"data_path\")\n",
        "model.fit(100)"
      ],
      "metadata": {
        "id": "_Vdo0ooomdrx",
        "outputId": "37dd30a7-5a4e-48cc-f496-2a5141c3258b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 27.0912 - accuracy: 0.0000e+00\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 14.8045 - accuracy: 0.1714\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 10.5362 - accuracy: 0.1714\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 9.2796 - accuracy: 0.3714\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 8.4270 - accuracy: 0.3714\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 7.6540 - accuracy: 0.3714\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 6.9816 - accuracy: 0.3714\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 6.2826 - accuracy: 0.3714\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 5.7572 - accuracy: 0.3714\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 5.3012 - accuracy: 0.3714\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 4.8584 - accuracy: 0.3714\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 4.4404 - accuracy: 0.3714\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 4.0720 - accuracy: 0.3714\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 3.7194 - accuracy: 0.3714\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 3.4042 - accuracy: 0.3714\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 3.1331 - accuracy: 0.3714\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.8559 - accuracy: 0.3714\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.7408 - accuracy: 0.3714\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.6466 - accuracy: 0.3714\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.4519 - accuracy: 0.3714\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.3645 - accuracy: 0.3714\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.2586 - accuracy: 0.3714\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.1901 - accuracy: 0.3714\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.1904 - accuracy: 0.3714\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.1854 - accuracy: 0.3714\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.2080 - accuracy: 0.3714\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.1696 - accuracy: 0.3714\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.1647 - accuracy: 0.3714\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.1465 - accuracy: 0.3714\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.1411 - accuracy: 0.3714\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2.1438 - accuracy: 0.3714\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.1529 - accuracy: 0.3714\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 2.1575 - accuracy: 0.3714\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.1541 - accuracy: 0.3714\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2.1479 - accuracy: 0.3714\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.1434 - accuracy: 0.3714\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.1406 - accuracy: 0.3714\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.1382 - accuracy: 0.3714\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.1353 - accuracy: 0.3714\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.1322 - accuracy: 0.3714\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.1301 - accuracy: 0.3714\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.1298 - accuracy: 0.3714\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.1311 - accuracy: 0.3714\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 2.1317 - accuracy: 0.3714\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2.1300 - accuracy: 0.3714\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.1270 - accuracy: 0.3714\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.1245 - accuracy: 0.3714\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.1226 - accuracy: 0.3714\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.1208 - accuracy: 0.3714\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2.1184 - accuracy: 0.3714\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.1178 - accuracy: 0.3714\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.1140 - accuracy: 0.3714\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.1122 - accuracy: 0.3714\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.1109 - accuracy: 0.3714\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.1096 - accuracy: 0.3714\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.1080 - accuracy: 0.3714\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.1058 - accuracy: 0.3714\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.1034 - accuracy: 0.3714\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.1011 - accuracy: 0.3714\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.0989 - accuracy: 0.3714\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.0966 - accuracy: 0.3714\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.0941 - accuracy: 0.3714\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.0914 - accuracy: 0.3714\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.0887 - accuracy: 0.3714\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.0862 - accuracy: 0.3714\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.0834 - accuracy: 0.3714\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.0799 - accuracy: 0.3714\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.0763 - accuracy: 0.3714\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.0729 - accuracy: 0.3714\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.0693 - accuracy: 0.3714\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.0654 - accuracy: 0.3714\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.0611 - accuracy: 0.3714\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.0566 - accuracy: 0.3714\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.0516 - accuracy: 0.3714\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.0467 - accuracy: 0.3714\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.0411 - accuracy: 0.3714\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.0348 - accuracy: 0.3714\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2.0286 - accuracy: 0.3714\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2.0219 - accuracy: 0.3714\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.0146 - accuracy: 0.3714\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.0071 - accuracy: 0.3714\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.9992 - accuracy: 0.3714\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.9902 - accuracy: 0.3714\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.9802 - accuracy: 0.3714\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.9705 - accuracy: 0.3714\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.9585 - accuracy: 0.3714\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.9475 - accuracy: 0.3714\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.9353 - accuracy: 0.3714\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 1.9210 - accuracy: 0.3714\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 1.9121 - accuracy: 0.3714\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 1.8929 - accuracy: 0.3714\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 1.8778 - accuracy: 0.3714\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.8613 - accuracy: 0.3714\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.8493 - accuracy: 0.3714\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.8360 - accuracy: 0.3714\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.8270 - accuracy: 0.3714\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.8144 - accuracy: 0.3714\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.8030 - accuracy: 0.3714\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.7917 - accuracy: 0.3714\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.7850 - accuracy: 0.3714\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}