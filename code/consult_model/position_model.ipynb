{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rb58853/ML-RSI-Images/blob/main/code/consult_model/position_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "Lm8_tPwj8Bbe"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Input, Dense, Reshape, Flatten,LSTM\n",
        "from keras.models import Model,Sequential\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MyModel():\n",
        "  def __init__(self, data_path, max_input_len = 50, max_output_len = 50):\n",
        "    self.max_input_len = max_input_len\n",
        "    self.max_output_len = max_output_len\n",
        "\n",
        "    self.data = self.get_data(data_path)\n",
        "    self.model = self.get_model()\n",
        "\n",
        "  def get_data(self,data_path):\n",
        "    return {\n",
        "    'input':[\n",
        "        'a dog to the left of a cat',\n",
        "        'a cat to the left of a lion',\n",
        "        'a bed to the left of a table',\n",
        "        'a sit to the left of a mantis',\n",
        "        'a object to the left of another object',\n",
        "        'a rock to the left of the water',\n",
        "        'a sea to the left of a shark'\n",
        "        ],\n",
        "    'output':[\n",
        "        ['a cat', 'a dog', '','',''],\n",
        "        ['a lion', 'a cat', '','',''],\n",
        "        ['a table', 'a bed', '','',''],\n",
        "        ['a mantis', 'a sit', '','',''],\n",
        "        ['another object', 'a object', '','',''],\n",
        "        ['the water', 'a rock', '','',''],\n",
        "        ['a shark', 'a sea', '','',''],\n",
        "        ],\n",
        "    'types':\n",
        "     [\n",
        "        [1,5,1,1,10,1,1,5],\n",
        "        [1,5,1,1,10,1,1,5],\n",
        "        [1,5,1,1,10,1,1,5],\n",
        "        [1,5,1,1,10,1,1,5],\n",
        "        [1,5,1,1,10,1,5,5],\n",
        "        [1,5,1,1,10,1,1,5],\n",
        "        [1,5,1,1,10,1,1,5],\n",
        "        ]\n",
        "      }\n",
        "\n",
        "  def get_train(self):\n",
        "    # Crea un objeto Tokenizer\n",
        "    tokenizer = Tokenizer()\n",
        "    # Ajusta el tokenizer a tus datos\n",
        "    types =  self.data['types']\n",
        "    output = self.data ['output']\n",
        "    input =  self.data ['input']\n",
        "\n",
        "    tokenizer.fit_on_texts(input+[':'])\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    sequences_input = tokenizer.texts_to_sequences(input)\n",
        "\n",
        "    sequences_output = []\n",
        "    for item in output:\n",
        "      texts = [text for text in item]\n",
        "      sequences_output.append(tokenizer.texts_to_sequences(texts))\n",
        "\n",
        "    train_x =  [[date_input, date_type] for date_input, date_type in zip(sequences_input, types)]\n",
        "    # train_x = np.array(train_x)\n",
        "    train_y = [item for item in sequences_output]\n",
        "\n",
        "    for case_ in train_x:\n",
        "      for item in case_:\n",
        "        while len(item) < self.max_input_len:\n",
        "          item.append(-1)\n",
        "\n",
        "    for case_ in train_y:\n",
        "      for item in case_:\n",
        "        while len(item) < self.max_output_len:\n",
        "          item.append(-1)\n",
        "\n",
        "    return  {'train_x':train_x, 'train_y':train_y}\n",
        "\n",
        "  def get_model(self):\n",
        "    total_len_output = self.max_output_len * 5\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(2, self.max_input_len)))\n",
        "    model.add(LSTM(256, activation='relu', return_sequences=True, input_shape=(100, self.max_input_len)))\n",
        "    model.add(LSTM(256, activation='relu', return_sequences=True, input_shape=(100, self.max_input_len)))\n",
        "    model.add(LSTM(256, activation='relu', return_sequences=True, input_shape=(100, self.max_input_len)))\n",
        "    model.add(LSTM(256, activation='relu'))\n",
        "    # model.add(Dense(128, activation='relu'))\n",
        "    # model.add(Dense(64, activation='relu'))\n",
        "    model.add(Flatten(input_shape=(0,64)))\n",
        "    model.add(Dense(total_len_output, activation='relu'))\n",
        "    model.add(Reshape((5, self.max_output_len)))\n",
        "    return model\n",
        "\n",
        "  def fit(self,epochs, batch_size=32):\n",
        "    self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    train = self.get_train()\n",
        "    train_x = train['train_x']\n",
        "    train_y = train['train_y']\n",
        "    self.model.fit(train_x, train_y, epochs=epochs, batch_size=32)\n",
        "    # Convierte tus datos en secuencias de nÃºmeros\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\"data_path\")\n",
        "\n",
        "model.fit(100)"
      ],
      "metadata": {
        "id": "_Vdo0ooomdrx",
        "outputId": "70126e53-90eb-496f-b902-dfbf4dcceb46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 6s 6s/step - loss: -405.1151 - accuracy: 0.0000e+00\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 77ms/step - loss: -603.7340 - accuracy: 0.0571\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 81ms/step - loss: -645.1660 - accuracy: 0.2000\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 79ms/step - loss: -668.1833 - accuracy: 0.3429\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 70ms/step - loss: -686.3841 - accuracy: 0.5714\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 71ms/step - loss: -693.0478 - accuracy: 0.5714\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 75ms/step - loss: -696.8556 - accuracy: 0.5714\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 76ms/step - loss: -696.1592 - accuracy: 0.5714\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 78ms/step - loss: -698.6837 - accuracy: 0.5714\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 83ms/step - loss: -707.4380 - accuracy: 0.5714\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 77ms/step - loss: -718.0571 - accuracy: 0.5714\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 60ms/step - loss: -724.1442 - accuracy: 0.5714\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -725.1440 - accuracy: 0.5714\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -726.8660 - accuracy: 0.5714\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -728.2127 - accuracy: 0.5714\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -728.0591 - accuracy: 0.5714\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -728.4686 - accuracy: 0.5714\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -728.6994 - accuracy: 0.5714\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -730.9052 - accuracy: 0.5714\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -731.2335 - accuracy: 0.5714\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -731.3900 - accuracy: 0.5714\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -731.6032 - accuracy: 0.5714\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -732.0435 - accuracy: 0.5714\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -734.2722 - accuracy: 0.5714\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 45ms/step - loss: -734.4792 - accuracy: 0.5714\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -734.7835 - accuracy: 0.5714\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -736.2330 - accuracy: 0.5714\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -739.9282 - accuracy: 0.5714\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -741.3812 - accuracy: 0.5714\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -744.6772 - accuracy: 0.5714\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -744.5659 - accuracy: 0.5714\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -744.8234 - accuracy: 0.5714\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -744.6173 - accuracy: 0.5714\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -744.9263 - accuracy: 0.5714\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -744.9935 - accuracy: 0.5714\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -745.1013 - accuracy: 0.5714\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 62ms/step - loss: -745.4095 - accuracy: 0.5714\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -747.2482 - accuracy: 0.5714\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -747.5698 - accuracy: 0.5714\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -747.6358 - accuracy: 0.5714\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -747.7508 - accuracy: 0.5714\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -748.4968 - accuracy: 0.5714\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -750.1689 - accuracy: 0.5714\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -748.8510 - accuracy: 0.5714\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -750.1827 - accuracy: 0.5714\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 55ms/step - loss: -750.1997 - accuracy: 0.5714\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -749.9962 - accuracy: 0.5714\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -750.2549 - accuracy: 0.5714\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -750.2963 - accuracy: 0.5714\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 46ms/step - loss: -750.3533 - accuracy: 0.5714\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -750.4378 - accuracy: 0.5714\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 57ms/step - loss: -750.5913 - accuracy: 0.5714\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -752.6097 - accuracy: 0.5714\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 47ms/step - loss: -752.9266 - accuracy: 0.5714\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 55ms/step - loss: -752.9391 - accuracy: 0.5714\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -752.9476 - accuracy: 0.5714\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -752.4694 - accuracy: 0.5714\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -752.9584 - accuracy: 0.5714\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -752.9700 - accuracy: 0.5714\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -752.9886 - accuracy: 0.5714\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -753.0113 - accuracy: 0.5714\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -752.7759 - accuracy: 0.5714\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -753.0595 - accuracy: 0.5714\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 60ms/step - loss: -753.0875 - accuracy: 0.5714\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -753.1215 - accuracy: 0.5714\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -753.1644 - accuracy: 0.5714\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -753.2209 - accuracy: 0.5714\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -753.2998 - accuracy: 0.5714\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -753.4249 - accuracy: 0.5714\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -753.7634 - accuracy: 0.5714\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 51ms/step - loss: -756.0495 - accuracy: 0.5714\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -756.1705 - accuracy: 0.5714\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -756.4349 - accuracy: 0.5714\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -758.9188 - accuracy: 0.5714\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -759.2872 - accuracy: 0.5714\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -761.4600 - accuracy: 0.5714\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 52ms/step - loss: -761.4656 - accuracy: 0.5714\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -760.8587 - accuracy: 0.5714\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 60ms/step - loss: -761.4672 - accuracy: 0.5714\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 72ms/step - loss: -761.4540 - accuracy: 0.5714\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 63ms/step - loss: -761.4583 - accuracy: 0.5714\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 60ms/step - loss: -759.0397 - accuracy: 0.5714\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -761.5124 - accuracy: 0.5714\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 59ms/step - loss: -760.7499 - accuracy: 0.5714\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 55ms/step - loss: -761.5593 - accuracy: 0.5714\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 60ms/step - loss: -761.5840 - accuracy: 0.5714\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 55ms/step - loss: -761.3126 - accuracy: 0.5714\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 53ms/step - loss: -761.1104 - accuracy: 0.5714\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 61ms/step - loss: -761.7314 - accuracy: 0.5714\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -761.8783 - accuracy: 0.5714\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -764.0031 - accuracy: 0.5714\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -764.2432 - accuracy: 0.5714\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 49ms/step - loss: -764.2446 - accuracy: 0.5714\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 50ms/step - loss: -764.2472 - accuracy: 0.5714\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 48ms/step - loss: -764.2507 - accuracy: 0.5714\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 81ms/step - loss: -764.2543 - accuracy: 0.5714\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 79ms/step - loss: -764.2579 - accuracy: 0.5714\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 102ms/step - loss: -764.2609 - accuracy: 0.5714\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 94ms/step - loss: -764.2628 - accuracy: 0.5714\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 107ms/step - loss: -764.2635 - accuracy: 0.5714\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}