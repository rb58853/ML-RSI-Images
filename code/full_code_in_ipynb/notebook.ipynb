{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rb58853/images_RIS-ML-Conv-NLP/blob/main/end_model/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnVhTeQkdLdO",
        "outputId": "a6dd296f-14f6-4190-d0e8-08960167cefc",
        "vscode": {
          "languageId": "bat"
        }
      },
      "outputs": [],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6OaGHeSdLdT"
      },
      "source": [
        "### Importar bibliotecas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gY5w4189dLdU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31zOxBhIQlGK"
      },
      "source": [
        "Si estas usando colab ejecute la siguiente celda para importar imagenes de prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LE5GmXC0QlGL",
        "outputId": "168e7e2f-c241-4209-902d-2d9644c1967c"
      },
      "outputs": [],
      "source": [
        "!mkdir images\n",
        "!wget -P images https://raw.githubusercontent.com/rb58853/images_RIS-ML-Conv-NLP/main/images/image_1.jpg\n",
        "!wget -P images https://raw.githubusercontent.com/rb58853/images_RIS-ML-Conv-NLP/main/images/image_2.jpg\n",
        "!wget -P images https://raw.githubusercontent.com/rb58853/images_RIS-ML-Conv-NLP/main/images/image_3.jpg\n",
        "!wget -P images https://raw.githubusercontent.com/rb58853/images_RIS-ML-Conv-NLP/main/images/image_4.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp5bLjhidLdV"
      },
      "source": [
        "### Cargar Imagen\n",
        "Cargar la imagen que se desea procesar. Para seleccionar una imagen distinta debe cambiar el valor de la variable `img_url`. Si desea cambiar el tamaño minimo que puede tener una segmentacion debe cambiar el valor de la variable `image_partition`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W32WdDIddLdV"
      },
      "outputs": [],
      "source": [
        "img_url = '/content/images/image_2.jpg'\n",
        "raw_image = Image.open(img_url).convert(\"RGB\")\n",
        "image = cv2.imread(img_url)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "image_partition = 64 #tamaño mínimo(en píxeles) de un cuadro de segmentación = tamaño(imagen)/image_partition\n",
        "weigth, heigth = raw_image.size\n",
        "area = (weigth * heigth)/image_partition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Seleccione el modelo de blip que desea utilizar, si esta en colab, para correr con el modelo blip 2 necesita una GPU v100 o superior. Se recomienda probar el modelo con `blip_version = 1` si no posee colab pro, o no posee al menos 16GB en su tarjeta grafica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "blip_version = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J531WAam-H5V"
      },
      "source": [
        "Para visualizar la imagen cargada ejecutar celda:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "h3U4sica-Fab",
        "outputId": "b728466f-b8a0-40b7-e1a9-fb275a5de59f"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ4vpueydLdW"
      },
      "source": [
        "# Cargar Modelos\n",
        "Para cargar y ejecutar todos los modelos y funciones necesarias para procesar la imagen ejecutar las celdas que estan comprendidas en este encabezado.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V5u9_PXdLdW"
      },
      "source": [
        "## Segment Anything Model\n",
        "Modelo de segmentación y funciones adicionales para procesar las salidas y entradas del mismo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZaNgE4LdLdX",
        "outputId": "2d93a414-f6ba-4ec3-f4bc-0d497d124879"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"Torchvision version:\", torchvision.__version__)\n",
        "print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "import sys\n",
        "!{sys.executable} -m pip install opencv-python matplotlib\n",
        "!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
        "\n",
        "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "model_type = \"vit_h\"\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "\n",
        "mask_generator = SamAutomaticMaskGenerator(sam)\n",
        "\n",
        "mask_generator_2 = SamAutomaticMaskGenerator(\n",
        "    model=sam,\n",
        "    points_per_side=32,\n",
        "    pred_iou_thresh=0.86,\n",
        "    stability_score_thresh=0.92,\n",
        "    crop_n_layers=1,\n",
        "    crop_n_points_downscale_factor=2,\n",
        "    min_mask_region_area=100,  # Requires open-cv to run post-processing\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UlxZN6-dLdX"
      },
      "source": [
        "#### Generar imágenes con las máscaras\n",
        "Funciones para tratar con las máscaras de salida del modelo y generar imágenes usando esta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivl0wnEDdLdY"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def mask_image(mask, raw_image, bbox):\n",
        "    weigth, heigth = raw_image.size\n",
        "    new_image = Image.new('RGBA', (weigth, heigth), (0, 0, 0, 0))\n",
        "\n",
        "    original_pixles = raw_image.load()\n",
        "    pixels = new_image.load()\n",
        "\n",
        "    for i in range (heigth):\n",
        "        for j in range (weigth):\n",
        "            if mask[i,j]:\n",
        "                pixels[j, i] = original_pixles[j,i]\n",
        "            else:\n",
        "                pass\n",
        "\n",
        "    x,y,w,h =  bbox[0],bbox[1],bbox[2],bbox[3]\n",
        "    return new_image.crop((x,y,x+w,y+h))\n",
        "\n",
        "def mask_caption(mask, raw_image,bbox):\n",
        "    return blip(mask_image(mask, raw_image,bbox))\n",
        "\n",
        "def bbox_image(bbox, image):\n",
        "    x,y,w,h =  bbox[0],bbox[1],bbox[2],bbox[3]\n",
        "    return image[y:y+h, x:x+w]\n",
        "\n",
        "def bbox_caption(bbox, image):\n",
        "    return blip(bbox_image(bbox, image))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsWiSrhCdLdY"
      },
      "outputs": [],
      "source": [
        "def all_areas_from_image(image, raw_image, min_area = 0, min_box_area = 0):\n",
        "    \"\"\"\n",
        "    INPUTS:\\n\n",
        "    `image`: imagen cargada con cv2 \\n\n",
        "    `raw_image`: imagen cargada con PIL.Image \\n\n",
        "    `min_area`: area minima en pixeles de tamaño que puede puede tener las imagenes segmentadas \\n\n",
        "    `min_box_area`: area minima en pixeles de tamaño que puede puede tener un cuadro que contiene una imagen segmentada \\n\n",
        "\n",
        "    OUTPUTS: \\n\n",
        "    `dict` = \\n\n",
        "    `{` \\n\n",
        "      'box': imagenes(cuadro comprendido en segmentacion), \\n\n",
        "      'mask': imagenes(solo segmentacion fondo transparente) \\n\n",
        "    `}` \\n\n",
        "    \"\"\"\n",
        "    masks = mask_generator_2.generate(image)\n",
        "    images_box= []\n",
        "    images_mask= []\n",
        "    for mask in masks:\n",
        "        box_im = bbox_image(mask['bbox'],image)\n",
        "        h, w, c = box_im.shape\n",
        "        box_area = h * w\n",
        "        if box_area >= min_box_area:\n",
        "            images_box.append(box_im)\n",
        "        if mask['area'] >= min_area:\n",
        "            images_mask.append(mask_image(mask['segmentation'], raw_image, mask['bbox']))\n",
        "    return {'box':images_box, 'mask':images_mask}\n",
        "\n",
        "def all_masks_from_sam(image, min_area = 0, min_box_area = 0):\n",
        "    masks = mask_generator_2.generate(image)\n",
        "    _masks = [mask for mask in masks]\n",
        "    index = 0\n",
        "    for mask in masks:\n",
        "        bbox = mask['bbox']\n",
        "        x,y,w,h =  bbox[0],bbox[1],bbox[2],bbox[3]\n",
        "        box_area = h * w\n",
        "        if box_area < min_box_area or mask['area'] < min_area:\n",
        "           _masks.remove(mask)\n",
        "           index -=1\n",
        "        index+=1\n",
        "    return _masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7woGwmzdLdZ"
      },
      "source": [
        "## BLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tx4X2z7EZOps"
      },
      "outputs": [],
      "source": [
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "\n",
        "if blip_version == 1:\n",
        "    blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "    blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c8deaaaab58b48e98bf2476bf041d5f9",
            "404cb9a52cf948d4bb63bb593c8e4bb4",
            "fdfea0b0b30b40a185825009db5c7c10",
            "832ea7f84b88477ab83e784c0d3fe857",
            "8bf194bc3a6246159cc0be3d000c5bde",
            "f216e39aad74420a8be895632d470d7a",
            "e0f305ec36b84d55adfa4a6a148e8a1c",
            "6a031288f56744ecb574aa3e39e6d3f2",
            "fbede12a3eaf49f6b18af18f0133014e",
            "bfe9cac28fdb4ab5a708b75f255089c1",
            "a1b1012874934665ab3fc04ef1af55e7"
          ]
        },
        "id": "rwT5imf5QlGS",
        "outputId": "16fd7e3c-a211-466a-dd5c-671204a95fbe"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "if blip_version == 2:\n",
        "    blip2_processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "    blip2_model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    blip2_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGpRT3RXdLdZ"
      },
      "outputs": [],
      "source": [
        "def blip (image):\n",
        "    if blip_version == 1:\n",
        "        return blip1(image)\n",
        "    else:\n",
        "        return blip2(image)\n",
        "\n",
        "def blip2 (image):\n",
        "    inputs = blip2_processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "    generated_ids = blip2_model.generate(**inputs, max_new_tokens=20)\n",
        "    generated_text = blip2_processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "    return generated_text\n",
        "\n",
        "def blip1 (_image):\n",
        "    inputs = blip_processor(_image, return_tensors=\"pt\").to(\"cuda\")\n",
        "    out = blip_model.generate(**inputs)\n",
        "    result = blip_processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "    if result[:9] == \"there is \":\n",
        "        result = result[9:]\n",
        "\n",
        "    return result\n",
        "\n",
        "def all_captions(image, raw_image, segmentation = 'box', min_area = 0, min_box_area = 0):\n",
        "    \"\"\"\n",
        "    INPUTS:\\n\n",
        "    `image`: imagen cargada con cv2 \\n\n",
        "    `raw_image`: imagen cargada con PIL.Image\\n\n",
        "    `segmentation`: tipo de segmentacion que se va a utilizar para seleccionar imagenes ('box' o 'mask')\\n\n",
        "    `min_area`: area minima en pixeles de tamaño que puede puede tener las imagenes segmentadas \\n\n",
        "    `min_box_area`: area minima en pixeles de tamaño que puede puede tener un cuadro que contiene una imagen segmentada \\n\n",
        "\n",
        "    OUTPUTS: \\n\n",
        "    `list` = `[`lista con cada una de las descripciones de las imagenes segmentadas agregada al a descripcion principal`]`\n",
        "    \"\"\"\n",
        "    origin = str(blip(raw_image))\n",
        "    captions = [origin]\n",
        "\n",
        "    if not less_use_memory:\n",
        "        areas = all_areas_from_image(image, raw_image, min_area,min_box_area)[segmentation]\n",
        "        for im in areas:\n",
        "            captions.append(origin +\" \"+ str(blip(im)))\n",
        "    else:\n",
        "        masks = all_masks_from_sam(image,min_area = min_area, min_box_area = min_box_area)\n",
        "        for mask in masks:\n",
        "            if segmentation == 'box':\n",
        "                captions.append(origin +\" \"+ str(bbox_caption(mask['bbox'], image)))\n",
        "            else:\n",
        "                captions.append(origin +\" \"+ str(mask_caption(mask['segmentation'], raw_image, mask['bbox'])))\n",
        "    return captions\n",
        "\n",
        "def all_captions_from_list_images(image, segmented_images):\n",
        "    origin = str(blip(raw_image))\n",
        "    captions = [origin]\n",
        "    for im in segmented_images:\n",
        "        caption = str(blip(im))\n",
        "        captions.append(origin +\" \"+ caption)\n",
        "    return captions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPlHAjTbdLdZ"
      },
      "source": [
        "## CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNZ9Qw3mdLda"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "clip_model = clip_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwS4AgTVdLda"
      },
      "outputs": [],
      "source": [
        "def select_caption(captions, image):\n",
        "    \"\"\"\n",
        "    Usando el modelo CLIP selecciona la mejor descripcion de la lista de descripciones\n",
        "\n",
        "    INPUTS:\\n\n",
        "    `captions`: lista de descripciones(texto) \\n\n",
        "    `image`: imagen a la cual hallar similitud con los textos\\n\n",
        "\n",
        "    OUTPUTS:\\n\n",
        "    `dict`:\\n\n",
        "    `{`\\n\n",
        "    'caption': texto con mayor similitud con la imagen (`str`)\\n\n",
        "    'probs': la lista de probabilidades que devuelve CLIP para esa imagen y textos(`list`) \\n\n",
        "    `}`\n",
        "    \"\"\"\n",
        "    inputs = clip_processor(text=captions, images=image, return_tensors=\"pt\", padding=True)\n",
        "    inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
        "    outputs = clip_model(**inputs)\n",
        "\n",
        "    logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
        "    probs = logits_per_image.softmax(dim=1)\n",
        "    return {'caption':select_from_probs(probs, captions), 'probs': probs[0]}\n",
        "\n",
        "def select_from_probs(probs, captions):\n",
        "    max_prob = 0\n",
        "    index = 0\n",
        "    for i,prob in zip(range(len(probs[0])),probs[0]):\n",
        "        if prob > max_prob:\n",
        "            max_prob = prob\n",
        "            index = i\n",
        "    return captions[index]\n",
        "\n",
        "def reduce_caption(caption, image):\n",
        "    \"\"\"\n",
        "    Dada una descripcion se procesa la misma eliminando palabras innecesarias, se decide si la palabra es necesaria o no hallando similitud con CLIP\\n\n",
        "\n",
        "    INPUTS:\\n\n",
        "    `caption`: texto que se desea procesar\\n\n",
        "    `image`: imagen con la cual se desea hallar la similitud\\n\n",
        "\n",
        "    OUTPUTS:\\n\n",
        "    `str`: Nueva descripcion con palabras eliminadas\n",
        "    \"\"\"\n",
        "    split = caption.split(' ')\n",
        "    for word in split:\n",
        "        temp = caption.split(' ')\n",
        "        temp.remove(word)\n",
        "        temp = ' '.join(temp)\n",
        "\n",
        "        inputs = clip_processor(text=[temp, caption], images=image, return_tensors=\"pt\", padding=True)\n",
        "        inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
        "        outputs = clip_model(**inputs)\n",
        "        logits_per_image = outputs.logits_per_image\n",
        "        probs = logits_per_image.softmax(dim=1)\n",
        "\n",
        "        if probs[0][0]> probs[0][1]:\n",
        "            caption = temp\n",
        "\n",
        "\n",
        "    return caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBGiMWKJOn9U"
      },
      "outputs": [],
      "source": [
        "def short_captions(probs,captions):\n",
        "    '''\n",
        "    Ordena las descripciones usando como criterio de comparacion cual tiene mayor similitud con la imagen. Develve la lista de estos comop un diccionario `prob: caption` en orden de mejor similitud a peor similitud \\n\n",
        "    INPUTS:\\n\n",
        "    `probs`: lista de probabilidades en orden original \\n\n",
        "    `captions`: lista de descripciones en orden original\\n\n",
        "    OUTPUTS:\\n\n",
        "    `dict`= `{` porb(`float`): descripcion(`str`)`}`\n",
        "    '''\n",
        "    _probs = [prob.item() for prob in probs]\n",
        "    for i in range(len(captions)):\n",
        "        for j in range(i+1, len(captions)):\n",
        "            if _probs[j]>_probs[i]:\n",
        "                temp_p= _probs[i]\n",
        "                temp_c = captions[i]\n",
        "                _probs[i] = _probs[j]\n",
        "                captions[i] = captions[j]\n",
        "                _probs[j] = temp_p\n",
        "                captions[j] = temp_c\n",
        "    return {prob: caption for prob,caption in zip(_probs,captions) }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0T2tjeQdLda"
      },
      "source": [
        "# Run Model\n",
        "En estas celdas se ejecutan las funciones creadas y modelos cargados. Si las 3 celdas con un solo click, se mostrara el ranking de las 10 mejores descripciones encontradas y las descripciones de cada una de las imagenes segmentadas junto a su imagen correspondiente:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_7DW5zzQlGV"
      },
      "source": [
        "En esta celda usted puede cambiar los parametros que desee para variar los resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GC-uRC2QlGV"
      },
      "outputs": [],
      "source": [
        "# ['box' or 'mask'] DEFAULT: 'box'\n",
        "_segmentation = 'box'\n",
        "\n",
        "# `int` DEFAULT: 80\n",
        "image_partition = 80\n",
        "area = (weigth * heigth)/image_partition\n",
        "\n",
        "# `int` cantidad de descripciones a mostrar DEFAULT: 10\n",
        "count_rank = 10\n",
        "\n",
        "# [`True` or `False`] Usar la descripcion reducida DEFAULT: True\n",
        "use_reduc = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ze7wHoYydLda"
      },
      "outputs": [],
      "source": [
        "segm_images = all_areas_from_image(image, raw_image, min_box_area = area, min_area = area/2)[_segmentation]\n",
        "captions = all_captions_from_list_images(image, segm_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0PH0_GZfPp0",
        "outputId": "cf78e58c-930e-4c8a-bc79-b3af4439b99d"
      },
      "outputs": [],
      "source": [
        "image_caption = str(blip(image))\n",
        "print(\"image_caption: \" + image_caption)\n",
        "result = select_caption(captions, raw_image)\n",
        "print(\"original_caption: \"+str(result['caption']))\n",
        "rduced_caption = reduce_caption(result['caption'], raw_image)\n",
        "print(\"reduced_caption: \"+str(rduced_caption))\n",
        "\n",
        "if use_reduc and rduced_caption not in captions:\n",
        "    captions.append(rduced_caption)\n",
        "\n",
        "result = select_caption(captions, raw_image)\n",
        "probs = result['probs']\n",
        "end_captions = short_captions(probs, captions)\n",
        "\n",
        "long_caption = image_caption\n",
        "max_long = 2\n",
        "len_caption_image = len(image_caption)\n",
        "for caption,_ in zip(end_captions.values(),range(max_long)):\n",
        "    long_caption += \" \" + caption[len_caption_image:]\n",
        "\n",
        "print(\"long_caption: \"+str(long_caption))\n",
        "rduced_long_caption = reduce_caption(long_caption, raw_image)\n",
        "print(\"reduced_long_caption: \"+str(rduced_long_caption), end= '\\n\\n')\n",
        "\n",
        "print(\"ranking: \")\n",
        "end_key = None\n",
        "last_print = False\n",
        "for key,value,i in zip(end_captions.keys(),end_captions.values(),range(1000)):\n",
        "    last_print = True\n",
        "    if i < count_rank:\n",
        "        print(str(i+1)+\". {:.2f}\".format(key * 100) + \"%: \"+ str(value))\n",
        "        las_print = False\n",
        "    end_key = key\n",
        "\n",
        "if last_print:\n",
        "    print(\".\\n.\\n.\\n\"+str(len(end_captions.keys())) + \". {:.2f}\".format(end_key * 100) + \"%: \"+ str(end_captions[end_key]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQhoEC_HB_w6"
      },
      "source": [
        "# Ver segmentaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gqyob5Ejvi-n",
        "outputId": "e8e5378a-c32e-4f4d-858f-c10a45ed6c20"
      },
      "outputs": [],
      "source": [
        "show = True\n",
        "if show:\n",
        "    for im in segm_images:\n",
        "        plt.figure(figsize=(2,2))\n",
        "        plt.title(blip(im))\n",
        "        plt.imshow(im)\n",
        "        plt.axis('off')\n",
        "        plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "404cb9a52cf948d4bb63bb593c8e4bb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f216e39aad74420a8be895632d470d7a",
            "placeholder": "​",
            "style": "IPY_MODEL_e0f305ec36b84d55adfa4a6a148e8a1c",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "6a031288f56744ecb574aa3e39e6d3f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "832ea7f84b88477ab83e784c0d3fe857": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfe9cac28fdb4ab5a708b75f255089c1",
            "placeholder": "​",
            "style": "IPY_MODEL_a1b1012874934665ab3fc04ef1af55e7",
            "value": " 2/2 [00:31&lt;00:00, 16.58s/it]"
          }
        },
        "8bf194bc3a6246159cc0be3d000c5bde": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1b1012874934665ab3fc04ef1af55e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bfe9cac28fdb4ab5a708b75f255089c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8deaaaab58b48e98bf2476bf041d5f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_404cb9a52cf948d4bb63bb593c8e4bb4",
              "IPY_MODEL_fdfea0b0b30b40a185825009db5c7c10",
              "IPY_MODEL_832ea7f84b88477ab83e784c0d3fe857"
            ],
            "layout": "IPY_MODEL_8bf194bc3a6246159cc0be3d000c5bde"
          }
        },
        "e0f305ec36b84d55adfa4a6a148e8a1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f216e39aad74420a8be895632d470d7a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbede12a3eaf49f6b18af18f0133014e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fdfea0b0b30b40a185825009db5c7c10": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a031288f56744ecb574aa3e39e6d3f2",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fbede12a3eaf49f6b18af18f0133014e",
            "value": 2
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
