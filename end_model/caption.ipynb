{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = '/content/2.jpg'\n",
    "raw_image = Image.open(img_url).convert(\"RGB\")\n",
    "image = cv2.imread(img_url)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment Anything Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchvision\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Torchvision version:\", torchvision.__version__)\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "import sys\n",
    "!{sys.executable} -m pip install opencv-python matplotlib\n",
    "!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create image from mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def mask_image(mask, raw_image):\n",
    "    weigth, heigth = raw_image.size\n",
    "    new_image = Image.new('RGBA', (weigth, heigth), (0, 0, 0, 0))\n",
    "\n",
    "    original_pixles = raw_image.load()\n",
    "    pixels = new_image.load()\n",
    "\n",
    "    for i in range (heigth):\n",
    "        for j in range (weigth):\n",
    "            if mask[i,j]:\n",
    "                pixels[j, i] = original_pixles[j,i]\n",
    "            else:\n",
    "                pass\n",
    "    return new_image\n",
    "\n",
    "def bbox_image(bbox, image):\n",
    "    x,y,w,h =  bbox[0],bbox[1],bbox[2],bbox[3]\n",
    "    return image[y:y+h, x:x+w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_areas_from_image(image, raw_image):\n",
    "    masks = mask_generator.generate(image)\n",
    "    images_box= []\n",
    "    images_mask= []\n",
    "    for mask in masks:\n",
    "        images_box.append(bbox_image(mask['bbox'],image))\n",
    "        images_mask.append(mask_image(mask['segmentation'], raw_image))\n",
    "    return {'box':images_box, 'mask':images_mask}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(\"cuda\")\n",
    "\n",
    "def blip (_image):\n",
    "    inputs = processor(_image, return_tensors=\"pt\").to(\"cuda\")\n",
    "    out = model.generate(**inputs)\n",
    "    print(processor.decode(out[0], skip_special_tokens=True))\n",
    "    return processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "def all_captions(image, raw_image):\n",
    "    areas = all_areas_from_image(image, raw_image)['mask']\n",
    "    origin = str(blip(image))\n",
    "    captions = [origin]\n",
    "    for im in areas:\n",
    "        captions.append(origin + str(blip(im)))\n",
    "    return captions    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_caption(captions, image):\n",
    "    inputs = processor(text=captions, images=image, return_tensors=\"pt\", padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "    probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n",
    "    print(probs)\n",
    "    return select_from_probs(probs, captions)\n",
    "\n",
    "def select_from_probs(probs, captions):\n",
    "    max_prob = 0\n",
    "    index = 0\n",
    "    for i,prob in zip(range(len(probs)),probs):\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            index = i\n",
    "    return captions[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = all_areas_from_image(image, raw_image)\n",
    "for caption in captions:\n",
    "    print(caption)\n",
    "    \n",
    "result = select_caption(captions, image)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
