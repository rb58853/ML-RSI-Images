{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rb58853/images_RIS-ML-Conv-NLP/blob/main/end_model/caption.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "bat"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnVhTeQkdLdO",
        "outputId": "8732b01e-dafb-476a-da75-5bc44176acf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6OaGHeSdLdT"
      },
      "source": [
        "## Import librarys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gY5w4189dLdU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp5bLjhidLdV"
      },
      "source": [
        "### Load Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "W32WdDIddLdV"
      },
      "outputs": [],
      "source": [
        "img_url = '/content/2.jpg'\n",
        "raw_image = Image.open(img_url).convert(\"RGB\")\n",
        "image = cv2.imread(img_url)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ4vpueydLdW"
      },
      "source": [
        "# Load all Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V5u9_PXdLdW"
      },
      "source": [
        "## Segment Anything Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZaNgE4LdLdX",
        "outputId": "87abd85a-94fd-405b-a477-6309899b9cf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.1.0+cu118\n",
            "Torchvision version: 0.16.0+cu118\n",
            "CUDA is available: True\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-iznk6jr8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-iznk6jr8\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "--2023-10-27 13:50:36--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.162.163.51, 3.162.163.11, 3.162.163.19, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.162.163.51|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2564550879 (2.4G) [binary/octet-stream]\n",
            "Saving to: ‘sam_vit_h_4b8939.pth.1’\n",
            "\n",
            "sam_vit_h_4b8939.pt 100%[===================>]   2.39G   200MB/s    in 17s     \n",
            "\n",
            "2023-10-27 13:50:53 (145 MB/s) - ‘sam_vit_h_4b8939.pth.1’ saved [2564550879/2564550879]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"Torchvision version:\", torchvision.__version__)\n",
        "print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "import sys\n",
        "!{sys.executable} -m pip install opencv-python matplotlib\n",
        "!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
        "\n",
        "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "model_type = \"vit_h\"\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "\n",
        "mask_generator = SamAutomaticMaskGenerator(sam)\n",
        "\n",
        "mask_generator_2 = SamAutomaticMaskGenerator(\n",
        "    model=sam,\n",
        "    points_per_side=32,\n",
        "    pred_iou_thresh=0.86,\n",
        "    stability_score_thresh=0.92,\n",
        "    crop_n_layers=1,\n",
        "    crop_n_points_downscale_factor=2,\n",
        "    min_mask_region_area=100,  # Requires open-cv to run post-processing\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UlxZN6-dLdX"
      },
      "source": [
        "#### Create image from mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ivl0wnEDdLdY"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def mask_image(mask, raw_image):\n",
        "    weigth, heigth = raw_image.size\n",
        "    new_image = Image.new('RGBA', (weigth, heigth), (0, 0, 0, 0))\n",
        "\n",
        "    original_pixles = raw_image.load()\n",
        "    pixels = new_image.load()\n",
        "\n",
        "    for i in range (heigth):\n",
        "        for j in range (weigth):\n",
        "            if mask[i,j]:\n",
        "                pixels[j, i] = original_pixles[j,i]\n",
        "            else:\n",
        "                pass\n",
        "    return new_image\n",
        "\n",
        "def bbox_image(bbox, image):\n",
        "    x,y,w,h =  bbox[0],bbox[1],bbox[2],bbox[3]\n",
        "    return image[y:y+h, x:x+w]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WsWiSrhCdLdY"
      },
      "outputs": [],
      "source": [
        "def all_areas_from_image(image, raw_image):\n",
        "    masks = mask_generator_2.generate(image)\n",
        "    # masks = mask_generator.generate(image)\n",
        "    images_box= []\n",
        "    images_mask= []\n",
        "    for mask in masks:\n",
        "        images_box.append(bbox_image(mask['bbox'],image))\n",
        "        images_mask.append(mask_image(mask['segmentation'], raw_image))\n",
        "    return {'box':images_box, 'mask':images_mask}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7woGwmzdLdZ"
      },
      "source": [
        "## BLIP"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "\n",
        "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(\"cuda\")"
      ],
      "metadata": {
        "id": "Tx4X2z7EZOps"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vGpRT3RXdLdZ"
      },
      "outputs": [],
      "source": [
        "def blip (_image):\n",
        "    inputs = blip_processor(_image, return_tensors=\"pt\").to(\"cuda\")\n",
        "    out = blip_model.generate(**inputs)\n",
        "    result = blip_processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "    if result[:9] == \"there is \":\n",
        "        result = result[9:]\n",
        "\n",
        "    return result\n",
        "\n",
        "def all_captions(image, raw_image):\n",
        "    # areas = all_areas_from_image(image, raw_image)['mask']\n",
        "    areas = all_areas_from_image(image, raw_image)['box']\n",
        "    origin = str(blip(raw_image))\n",
        "    captions = [origin]\n",
        "    for im in areas:\n",
        "        captions.append(origin +\" \"+ str(blip(im)))\n",
        "    return captions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPlHAjTbdLdZ"
      },
      "source": [
        "## CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xNZ9Qw3mdLda"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "clip_model = clip_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UwS4AgTVdLda"
      },
      "outputs": [],
      "source": [
        "def select_caption(captions, image):\n",
        "    inputs = clip_processor(text=captions, images=image, return_tensors=\"pt\", padding=True)\n",
        "    inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
        "    outputs = clip_model(**inputs)\n",
        "\n",
        "    logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
        "    probs = logits_per_image.softmax(dim=1)\n",
        "    return {'caption':select_from_probs(probs, captions), 'probs': probs[0]}\n",
        "\n",
        "def select_from_probs(probs, captions):\n",
        "    max_prob = 0\n",
        "    index = 0\n",
        "    for i,prob in zip(range(len(probs[0])),probs[0]):\n",
        "        if prob > max_prob:\n",
        "            max_prob = prob\n",
        "            index = i\n",
        "    return captions[index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0T2tjeQdLda"
      },
      "source": [
        "# Run Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def short_captions(probs,captions):\n",
        "    for i in range(len(captions)):\n",
        "        for j in range(i+1, len(captions)):\n",
        "            if probs[j]>probs[i]:\n",
        "                temp_p= probs[i]\n",
        "                temp_c = captions[i]\n",
        "                probs[i] = probs[j]\n",
        "                captions[i] = captions[j]\n",
        "                probs[j] = temp_p\n",
        "                captions[j] = temp_c\n",
        "    return {prob: caption for prob,caption in zip(probs,captions) }"
      ],
      "metadata": {
        "id": "KBGiMWKJOn9U"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ze7wHoYydLda",
        "outputId": "e4fd57dd-43e1-4ac9-96eb-48e6d207dbfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n",
            "entro a eliminar there is\n"
          ]
        }
      ],
      "source": [
        "captions = all_captions(image, raw_image)\n",
        "# for caption in captions:\n",
        "#     print(caption)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = select_caption(captions, raw_image)\n",
        "probs = result['probs']\n",
        "end_captions = short_captions(probs, captions)\n",
        "for key,value in zip(end_captions.keys(),end_captions.values()):\n",
        "    print(\"{:.2f}\".format(key.item() * 100) + \"%: \"+ str(value))\n",
        "\n",
        "print(\"\\nCaption: \"+str(result['caption']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0PH0_GZfPp0",
        "outputId": "21d9ddfe-753d-45d5-d224-63e7bfb54c30"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57.73%: a cat and a dog laying on a bed together a close up of a knife on a cutting board with a knife in it\n",
            "11.32%: a cat and a dog laying on a bed together a man holding a knife and fork in his hand\n",
            "6.42%: a cat and a dog laying on a bed together a close up of a person's face with a toothbrush\n",
            "5.17%: a cat and a dog laying on a bed together someone is cutting a dog's hair with a pair of scissors\n",
            "5.17%: a cat and a dog laying on a bed together a close up of a toothbrush with a tooth brush in it\n",
            "5.17%: a cat and a dog laying on a bed together blurry image of a cat looking at a person in a mirror\n",
            "5.17%: a cat and a dog laying on a bed together blurred image of a close up of a person's eye\n",
            "5.17%: a cat and a dog laying on a bed together a close up of a cat with a blurry background\n",
            "5.17%: a cat and a dog laying on a bed together a blurry photo of a red and white clock\n",
            "5.17%: a cat and a dog laying on a bed together a close up of a person with a red nose and a red nose\n",
            "5.17%: a cat and a dog laying on a bed together\n",
            "5.17%: a cat and a dog laying on a bed together a cat that is sitting on a table with a remote control\n",
            "5.17%: a cat and a dog laying on a bed together a cat that is laying down on a bed\n",
            "4.07%: a cat and a dog laying on a bed together blurry image of a close up of a toothbrush with a blurry background\n",
            "2.33%: a cat and a dog laying on a bed together a blurry photo of a toothbrush with a tooth brush in it\n",
            "2.33%: a cat and a dog laying on a bed together a blurry photo of a person holding a cell phone\n",
            "2.33%: a cat and a dog laying on a bed together a blurry photo of a man wearing a hat\n",
            "1.63%: a cat and a dog laying on a bed together araffe and a cat laying on a couch with a pair of scissors\n",
            "1.63%: a cat and a dog laying on a bed together blurry image of a cat with a collar on its head\n",
            "1.63%: a cat and a dog laying on a bed together a blurry photo of a person holding a cell phone\n",
            "1.63%: a cat and a dog laying on a bed together blurry image of a cat sitting on a bench in a room\n",
            "1.63%: a cat and a dog laying on a bed together blurred image of a fire hydrant with a blurry background\n",
            "1.63%: a cat and a dog laying on a bed together a blurry photo of a white and black cat\n",
            "1.63%: a cat and a dog laying on a bed together a cat that is sitting on a bed with a pillow\n",
            "1.63%: a cat and a dog laying on a bed together a banana that is sitting on a table\n",
            "1.63%: a cat and a dog laying on a bed together blurry image of a cat with a long tail and a long tail\n",
            "1.63%: a cat and a dog laying on a bed together a blurry image of a plane flying in the sky\n",
            "1.63%: a cat and a dog laying on a bed together blurry photo of a car with a red light on it\n",
            "1.63%: a cat and a dog laying on a bed together a close up of a plate of food with a sandwich on it\n",
            "1.63%: a cat and a dog laying on a bed together blurry image of a cat with a red hat on\n",
            "1.63%: a cat and a dog laying on a bed together blurred image of a blurry photo of a bird flying in the sky\n",
            "1.63%: a cat and a dog laying on a bed together a close up of a white and black cat\n",
            "1.63%: a cat and a dog laying on a bed together a bird that is sitting on a bed with a blanket\n",
            "1.63%: a cat and a dog laying on a bed together a blurry photo of a white and black cat\n",
            "1.63%: a cat and a dog laying on a bed together blurred image of a red and purple background with a blurry image of a clock\n",
            "1.63%: a cat and a dog laying on a bed together a cat that is sitting on a bed with a book\n",
            "0.96%: a cat and a dog laying on a bed together a close up of a cat with a blurry look on its face\n",
            "0.96%: a cat and a dog laying on a bed together blurry image of a woman with a cell phone in her hand\n",
            "0.96%: a cat and a dog laying on a bed together blurry image of a piano in a living room with a couch\n",
            "0.96%: a cat and a dog laying on a bed together blurry photo of a cat laying on a bed with a remote control\n",
            "0.96%: a cat and a dog laying on a bed together a cat that is sitting on a bed with a blanket\n",
            "0.96%: a cat and a dog laying on a bed together there are two cats that are sitting together in a red chair\n",
            "0.96%: a cat and a dog laying on a bed together a piece of bread on a cutting board on the table\n",
            "0.65%: a cat and a dog laying on a bed together blurry image of a person with a hand on a phone\n",
            "0.29%: a cat and a dog laying on a bed together blurry image of a blurry image of a person holding a cell phone\n",
            "0.25%: a cat and a dog laying on a bed together blurry image of a bird sitting on a branch in a tree\n",
            "0.10%: a cat and a dog laying on a bed together a small bird sitting on a branch in the air\n",
            "0.09%: a cat and a dog laying on a bed together a small bird that is sitting on a branch\n",
            "0.07%: a cat and a dog laying on a bed together a blurry photo of a person in bed with a teddy bear\n",
            "0.06%: a cat and a dog laying on a bed together a blurry photo of a car with a blue tail\n",
            "0.06%: a cat and a dog laying on a bed together someone is holding a blue ball in their hand\n",
            "0.06%: a cat and a dog laying on a bed together a cat that is sitting on a bed with a blanket\n",
            "0.06%: a cat and a dog laying on a bed together a cat that is sitting on a couch with a blue ball\n",
            "0.02%: a cat and a dog laying on a bed together a cat that is laying down on a bed\n",
            "0.02%: a cat and a dog laying on a bed together a cat that is laying down on a bed\n",
            "0.02%: a cat and a dog laying on a bed together a cat and a dog laying on a bed together\n",
            "0.02%: a cat and a dog laying on a bed together a man sitting on a bus with a banana\n",
            "0.02%: a cat and a dog laying on a bed together a blurry photo of a person holding a blue ball\n",
            "0.02%: a cat and a dog laying on a bed together a black cat sitting on a black suitcase\n",
            "0.02%: a cat and a dog laying on a bed together a cat and a dog laying on a couch together\n",
            "0.02%: a cat and a dog laying on a bed together a cat that is sitting on a bed with a blanket\n",
            "0.01%: a cat and a dog laying on a bed together a cat that is standing on a wooden floor\n",
            "\n",
            "Caption: a cat and a dog laying on a bed together a close up of a knife on a cutting board with a knife in it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for im in all_areas_from_image(image, raw_image)['box']:\n",
        "    plt.figure(figsize=(3,3))\n",
        "    plt.imshow(im)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "gqyob5Ejvi-n"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}